{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"tweets-train (1).csv\")\n",
    "df1=pd.read_csv(\"tweets-test.csv\")\n",
    "df2=pd.read_csv(\"tweets-valid.csv\")\n",
    "df3=pd.read_csv(\"tweets-extra.csv\")\n",
    "\n",
    "df = pd.concat([df, df1, df2, df3], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       " 1    7643\n",
       "-1    5447\n",
       " 0    5288\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.3-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\hpw\\appdata\\roaming\\python\\python311\\site-packages (from xgboost) (1.25.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\hpw\\appdata\\roaming\\python\\python311\\site-packages (from xgboost) (1.14.0)\n",
      "Downloading xgboost-2.1.3-py3-none-win_amd64.whl (124.9 MB)\n",
      "   ---------------------------------------- 0.0/124.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/124.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/124.9 MB 3.4 MB/s eta 0:00:37\n",
      "    --------------------------------------- 2.1/124.9 MB 3.7 MB/s eta 0:00:34\n",
      "   - -------------------------------------- 3.1/124.9 MB 4.2 MB/s eta 0:00:30\n",
      "   - -------------------------------------- 4.5/124.9 MB 4.7 MB/s eta 0:00:26\n",
      "   -- ------------------------------------- 6.3/124.9 MB 5.4 MB/s eta 0:00:23\n",
      "   -- ------------------------------------- 8.1/124.9 MB 5.9 MB/s eta 0:00:20\n",
      "   --- ------------------------------------ 10.5/124.9 MB 6.7 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 13.6/124.9 MB 7.6 MB/s eta 0:00:15\n",
      "   ----- ---------------------------------- 17.3/124.9 MB 8.7 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 21.0/124.9 MB 9.7 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 24.9/124.9 MB 10.5 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 30.1/124.9 MB 11.7 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 35.9/124.9 MB 12.8 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 42.2/124.9 MB 14.1 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 50.3/124.9 MB 15.6 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 58.7/124.9 MB 17.2 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 67.6/124.9 MB 18.7 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 76.8/124.9 MB 20.0 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 83.9/124.9 MB 20.7 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 93.3/124.9 MB 21.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 102.5/124.9 MB 23.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 109.6/124.9 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 118.8/124.9 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.8/124.9 MB 24.9 MB/s eta 0:00:01\n",
      "   --------------------------------------- 124.9/124.9 MB 23.8 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.735038084874864\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.75      0.76      0.76      1129\n",
      "           0       0.67      0.66      0.66      1040\n",
      "           1       0.77      0.77      0.77      1507\n",
      "\n",
      "    accuracy                           0.74      3676\n",
      "   macro avg       0.73      0.73      0.73      3676\n",
      "weighted avg       0.73      0.74      0.73      3676\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load your dataset\n",
    "\n",
    "tweets = df['tweet']\n",
    "labels = df['label']\n",
    "\n",
    "# Preprocess text (basic cleaning)\n",
    "import re\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+|pic\\.twitter\\.com\\S*', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\n+', ' ', text)  # Remove punctuation\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "tweets = tweets.apply(clean_text)\n",
    "\n",
    "# Vectorize text using TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X = tfidf.fit_transform(tweets).toarray()\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Predict on new data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.735854189336235\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.75      0.76      0.76      1129\n",
      "           0       0.67      0.65      0.66      1040\n",
      "           1       0.77      0.77      0.77      1507\n",
      "\n",
      "    accuracy                           0.74      3676\n",
      "   macro avg       0.73      0.73      0.73      3676\n",
      "weighted avg       0.74      0.74      0.74      3676\n",
      "\n",
      "Pipeline saved as 'tweet_sentiment_pipeline.pkl'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load Marathi stopwords\n",
    "try:\n",
    "    marathi_stopwords = set(stopwords.words(\"marathi\"))\n",
    "except LookupError:\n",
    "    import nltk\n",
    "    nltk.download(\"stopwords\")\n",
    "    marathi_stopwords = set(stopwords.words(\"marathi\"))\n",
    "\n",
    "# Additional stopwords (if needed)\n",
    "custom_stopwords = {\"तुम्ही\", \"आहे\", \"हे\", \"का\"}  # Add any custom stopwords here\n",
    "marathi_stopwords.update(custom_stopwords)\n",
    "\n",
    "# Load your dataset\n",
    "# Replace with your actual data loading code\n",
    "# Example: df = pd.read_csv(\"your_dataset.csv\")\n",
    "# Ensure df['tweet'] and df['label'] exist\n",
    "tweets = df['tweet']\n",
    "labels = df['label']\n",
    "\n",
    "# Preprocess text (cleaning and removing stopwords)\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+|pic\\.twitter\\.com\\S*', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove special characters\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    # Remove Marathi stopwords\n",
    "    text = ' '.join(word for word in text.split() if word not in marathi_stopwords)\n",
    "    return text\n",
    "\n",
    "tweets = tweets.apply(clean_text)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the pipeline\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Serialize the pipeline\n",
    "with open('tweet_sentiment_pipeline.pkl', 'wb') as f:\n",
    "    pickle.dump(pipeline, f)\n",
    "\n",
    "print(\"Pipeline saved as 'tweet_sentiment_pipeline.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['काल महापूर आल्यामुळे लोकांचे खूप नुकसान झाले.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.53618768, 0.32771876, 0.13609356]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tweet = [\"काल महापूर आल्यामुळे लोकांचे खूप नुकसान झाले.\"]\n",
    "new_tweet_cleaned = [clean_text(tweet) for tweet in new_tweet]\n",
    "new_tweet_vectorized = tfidf.transform(new_tweet_cleaned).toarray()\n",
    "predicted_label = model.predict(new_tweet_vectorized)\n",
    "print(new_tweet_cleaned)\n",
    "model.predict_proba(new_tweet_vectorized)\n",
    "# print(\"Predicted Sentiment:\", predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7848204570184983\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.80      0.82      0.81      1129\n",
      "           0       0.72      0.69      0.70      1040\n",
      "           1       0.82      0.82      0.82      1507\n",
      "\n",
      "    accuracy                           0.78      3676\n",
      "   macro avg       0.78      0.78      0.78      3676\n",
      "weighted avg       0.78      0.78      0.78      3676\n",
      "\n",
      "Pipeline saved as 'tweet_sentiment_pipeline.pkl'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pickle\n",
    "\n",
    "# Load your dataset\n",
    "# Assuming your dataset is loaded in a DataFrame named `df`\n",
    "# Replace with your actual data loading code\n",
    "# Example: df = pd.read_csv(\"your_dataset.csv\")\n",
    "tweets = df['tweet']\n",
    "labels = df['label']\n",
    "\n",
    "# Preprocess text (basic cleaning)\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+|pic\\.twitter\\.com\\S*', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\n+', ' ', text)  # Remove punctuation\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "tweets = tweets.apply(clean_text)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the pipeline\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Serialize the pipeline\n",
    "with open('tweet_sentiment_pipeline.pkl', 'wb') as f:\n",
    "    pickle.dump(pipeline, f)\n",
    "\n",
    "print(\"Pipeline saved as 'tweet_sentiment_pipeline.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('tweets-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = test_data['tweet']  # Extract text\n",
    "true_labels = test_data['label']  # Extract true labels\n",
    "\n",
    "texts_cleaned = [clean_text(tweet) for tweet in test_data['tweet']]\n",
    "X_test = tfidf .transform(texts_cleaned).toarray()\n",
    "predicted_labels = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7986666666666666\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.83      0.82      0.82       750\n",
      "     Neutral       0.81      0.74      0.78       750\n",
      "    Positive       0.76      0.83      0.80       750\n",
      "\n",
      "    accuracy                           0.80      2250\n",
      "   macro avg       0.80      0.80      0.80      2250\n",
      "weighted avg       0.80      0.80      0.80      2250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_labels, predicted_labels, target_names=['Negative', 'Neutral', 'Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model,open(\"model.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tfidf,open(\"tfidf.pkl\",\"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
